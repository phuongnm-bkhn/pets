{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-reuters.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lrXHjNKRAgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install pytorch \n",
        "# !pip install torchtext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmZP8XXnPsXx",
        "colab_type": "code",
        "outputId": "f2884527-4491-4d24-ac4b-3ae54043582c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import reuters\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=VOCAB_SIZE)\n",
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "# Note that our indices were offset by 3\n",
        "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
        "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
        "print(decoded_newswire)\n",
        "\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=VOCAB_SIZE):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "# Our vectorized training+test data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "# Our vectorized training+test label \n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)\n",
        "\n",
        "# split original train set into train + valid \n",
        "x_val = x_train[:300]\n",
        "partial_x_train = x_train[300:]\n",
        "y_val = one_hot_train_labels[:300]\n",
        "partial_y_train = one_hot_train_labels[300:]\n",
        "\n",
        "D_in, H, D_out = partial_x_train.shape[1], 128, len(set(train_labels))\n",
        "\n",
        "print(\"- size train/dev/test:\", len(partial_x_train), len(x_val), len(x_test))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n",
            "- size train/dev/test: 8682 300 2246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ryTwJoPO3vi",
        "colab_type": "code",
        "outputId": "08c35d26-1362-48c4-e641-59f804ba3fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import copy \n",
        "\n",
        "seed = 100\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(torch.cuda.get_device_name(device=device))\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla T4\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKO9dYYUuBCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReutersDataset(torch.utils.data.TensorDataset):\n",
        "    \"\"\"Reuters dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, data, label, device=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data (torch.Tensor): data sample using Tensor type.\n",
        "            label (torch.Tensor): label sample using Tensor type \n",
        "        \"\"\"\n",
        "        self.data = data.to(device) if device is not None else data\n",
        "        self.label = label if device is not None else label\n",
        "        self.size = len(self.data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.label[idx] \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpNkf60P03jW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "# import torch.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "class FFN(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        \"\"\"\n",
        "        In the constructor we construct three nn.Linear instances that we will use\n",
        "        in the forward pass.\n",
        "        \"\"\"\n",
        "        super(FFN, self).__init__()\n",
        "\n",
        "        self.linear1 = torch.nn.Linear(D_in, H)\n",
        "        self.activation1 = torch.nn.Tanh()\n",
        "\n",
        "        self.linear2 = torch.nn.Linear(H,  int(H/2))\n",
        "        self.activation2 = torch.nn.Tanh()\n",
        "\n",
        "        self.output_linear = torch.nn.Linear(int(H/2), D_out)\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "        self.apply(self._init_weights)\n",
        "    \n",
        "    @staticmethod\n",
        "    def _init_weights(m):\n",
        "        if type(m) == torch.nn.Linear:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            m.bias.data.fill_(0.00)\n",
        "            \n",
        "    def forward(self, x): \n",
        "        h1 = self.activation1(self.linear1(x))\n",
        "        h2 = self.activation2(self.linear2(h1))\n",
        "        return self.softmax(self.output_linear(h2))\n",
        "\n",
        "\n",
        "class RNNText(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, emb_dim, class_num, hidden_dim=300, bidirectional=True):\n",
        "\n",
        "    super(RNNText, self).__init__()\n",
        "\n",
        "    self.emb_dim = emb_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "    self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=1, bidirectional=bidirectional, batch_first=True)\n",
        "\n",
        "    self.hidden2out = nn.Linear(hidden_dim, class_num)\n",
        "    self.softmax = nn.LogSoftmax()\n",
        "\n",
        "    self.dropout_layer = nn.Dropout(p=0.2)\n",
        "\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    return(autograd.Variable(torch.randn(1, batch_size, self.hidden_dim)),\n",
        "            autograd.Variable(torch.randn(1, batch_size, self.hidden_dim)))\n",
        "\n",
        "\n",
        "  def forward(self, batch, lengths=None):\n",
        "    self.hidden = self.init_hidden(batch.size(-1))\n",
        "\n",
        "    embeds = self.embedding(batch)\n",
        "    packed_input = embeds #pack_padded_sequence(embeds, lengths)\n",
        "    outputs, (ht, ct) = self.lstm(packed_input)\n",
        "\n",
        "    # ht is the last hidden state of the sequences\n",
        "    # ht = (1 x batch_size x hidden_dim)\n",
        "    # ht[-1] = (batch_size x hidden_dim)\n",
        "    output = self.dropout_layer(ht[-1])\n",
        "    output = self.hidden2out(output)\n",
        "    output = self.softmax(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "class CNNText(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, emb_dim, class_num, channels_out=32, kernel_sizes=(3, 4, 5), dropout=0.05):\n",
        "        super(CNNText, self).__init__()\n",
        "        \n",
        "        V = vocab_size\n",
        "        D = emb_dim\n",
        "        C = class_num\n",
        "        Ci = 1\n",
        "        Co = channels_out\n",
        "        Ks = kernel_sizes\n",
        "\n",
        "        self.embed = nn.Embedding(V, D)\n",
        "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
        "        '''\n",
        "        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n",
        "        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n",
        "        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n",
        "        '''\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "    \n",
        "    @staticmethod\n",
        "    def _init_weights(m):\n",
        "        if type(m) == torch.nn.Linear:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            m.bias.data.fill_(0.00)\n",
        "        elif type(m) == torch.nn.Embedding:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    def conv_and_pool(self, x, conv):\n",
        "        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n",
        "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)  # (N, W, D)\n",
        "        \n",
        "        # if self.args.static:\n",
        "        #     x = Variable(x)\n",
        "\n",
        "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
        "\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
        "\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
        "\n",
        "        x = torch.cat(x, 1)\n",
        "\n",
        "        '''\n",
        "        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n",
        "        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n",
        "        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n",
        "        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n",
        "        '''\n",
        "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
        "        logit = self.fc1(x)  # (N, C)\n",
        "        return logit\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAOOYCkf1H2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# evaluate model in training phase or testing phase \n",
        "def evaluate(model_nn, data_loader):\n",
        "\n",
        "  # using no_grad to prevent the conflict in training phase\n",
        "  # in this setting, there is not any gradient computations \n",
        "  with torch.no_grad():\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    total_sample = 0\n",
        "    total_predictions = torch.LongTensor([]).to(device)\n",
        "    model_nn.eval()\n",
        "\n",
        "    # compute each minibatch in test/valid set\n",
        "    for mini_batch in data_loader:\n",
        "      # infer steps similar to training phase\n",
        "      inputs, labels = mini_batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      predictions = model_nn(inputs)\n",
        "      lb_predictions = torch.max(predictions, dim=1)[1]\n",
        "\n",
        "      # calculate loss and accuracy \n",
        "      loss = criterion(predictions, labels)\n",
        "      acc = (lb_predictions == labels).sum()\n",
        "\n",
        "      # gather all result for each mini_batch  \n",
        "      total_predictions = torch.cat((total_predictions, lb_predictions))\n",
        "      total_sample += len(labels)\n",
        "      total_loss += loss.item()\n",
        "      total_acc += acc.item()\n",
        "\n",
        "    return total_loss/len(data_loader), total_acc / total_sample, total_predictions\n",
        "\n",
        "\n",
        "def train(model, criterion, optimizer, trainloader, validloader, N_EPOCHS = 11):\n",
        "    # training  model with some epoch \n",
        "    for epoch in range(N_EPOCHS):  \n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            model.train()\n",
        "\n",
        "            # get the inputs and label \n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 200 == 199:    # print every 200 mini-batches\n",
        "                print('[Epoch %d/%d, Iter %5d] train loss: %.3f' %\n",
        "                      (epoch + 1, N_EPOCHS, i + 1, running_loss / 200))\n",
        "                running_loss = 0.0\n",
        "\n",
        "                # \n",
        "                valid_loss, valid_acc, _ = evaluate(model, validloader)\n",
        "                print('[Epoch %d, Iter %5d] valid loss: %.3f - acc: %.3f' %\n",
        "                      (epoch + 1, i + 1, valid_loss, valid_acc))\n",
        "    print('Finished Training')\n",
        "    return model \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EeMpWz-5cMK",
        "colab_type": "code",
        "outputId": "be1e16f5-e422-4ae8-f72d-91a0bc392b91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        }
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(ReutersDataset(torch.Tensor(partial_x_train),\n",
        "                                                         torch.LongTensor(train_labels[300:]), \n",
        "                                                         device=None), \n",
        "                                          batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "validloader = torch.utils.data.DataLoader(ReutersDataset(torch.Tensor(x_val),\n",
        "                                                         torch.LongTensor(train_labels[:300]), \n",
        "                                                         device=None), \n",
        "                                          batch_size=32,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(ReutersDataset(torch.Tensor(x_test),\n",
        "                                                        torch.LongTensor(test_labels), \n",
        "                                                        device=None), \n",
        "                                          batch_size=32,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Construct our model by instantiating the class defined above\n",
        "model = FFN(D_in, H, D_out)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# gpu load\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "\n",
        "# training \n",
        "model = train(model, criterion, optimizer, trainloader, validloader, N_EPOCHS = 11)\n",
        "\n",
        "# evaluate\n",
        "test_loss, test_acc, lb_predictions = evaluate(model, testloader)\n",
        "print('- test loss: %.3f \\n- test acc: %.3f' % (test_loss, test_acc*100))\n",
        "\n",
        "print(\"- 30 first predictions + gold labels:\")\n",
        "print(lb_predictions[:30])\n",
        "print(test_labels[:30])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFN(\n",
            "  (linear1): Linear(in_features=10000, out_features=128, bias=True)\n",
            "  (activation1): Tanh()\n",
            "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (activation2): Tanh()\n",
            "  (output_linear): Linear(in_features=64, out_features=46, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n",
            "[Epoch 1/11, Iter   200] train loss: 3.327\n",
            "[Epoch 1, Iter   200] valid loss: 3.201 - acc: 0.667\n",
            "[Epoch 2/11, Iter   200] train loss: 3.149\n",
            "[Epoch 2, Iter   200] valid loss: 3.143 - acc: 0.730\n",
            "[Epoch 3/11, Iter   200] train loss: 3.105\n",
            "[Epoch 3, Iter   200] valid loss: 3.108 - acc: 0.773\n",
            "[Epoch 4/11, Iter   200] train loss: 3.033\n",
            "[Epoch 4, Iter   200] valid loss: 3.058 - acc: 0.813\n",
            "[Epoch 5/11, Iter   200] train loss: 3.003\n",
            "[Epoch 5, Iter   200] valid loss: 3.054 - acc: 0.817\n",
            "[Epoch 6/11, Iter   200] train loss: 2.983\n",
            "[Epoch 6, Iter   200] valid loss: 3.034 - acc: 0.840\n",
            "[Epoch 7/11, Iter   200] train loss: 2.970\n",
            "[Epoch 7, Iter   200] valid loss: 3.033 - acc: 0.847\n",
            "[Epoch 8/11, Iter   200] train loss: 2.959\n",
            "[Epoch 8, Iter   200] valid loss: 3.035 - acc: 0.840\n",
            "[Epoch 9/11, Iter   200] train loss: 2.951\n",
            "[Epoch 9, Iter   200] valid loss: 3.043 - acc: 0.830\n",
            "[Epoch 10/11, Iter   200] train loss: 2.941\n",
            "[Epoch 10, Iter   200] valid loss: 3.045 - acc: 0.830\n",
            "[Epoch 11/11, Iter   200] train loss: 2.937\n",
            "[Epoch 11, Iter   200] valid loss: 3.032 - acc: 0.837\n",
            "Finished Training\n",
            "- test loss: 3.062 \n",
            "- test acc: 81.523\n",
            "- 30 first predictions + gold labels:\n",
            "tensor([ 3, 10,  1,  4, 13,  3,  3,  3,  3,  3,  1,  4,  1,  3,  1, 11,  3,  3,\n",
            "        19,  3,  8,  3,  3,  4,  9,  3,  4,  6, 10,  3], device='cuda:0')\n",
            "[ 3 10  1  4  4  3  3  3  3  3  5  4  1  3  1 11 23  3 19  3  8  3  3  3\n",
            "  9  3  4  6 10  3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYlRnYqlEVkU",
        "colab_type": "code",
        "outputId": "a3d619d1-9c50-4135-fc52-50fee9234cd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        }
      },
      "source": [
        "\n",
        "# define padding method \n",
        "def padding_word(mini_batch,  padding_id = 0, max_length=-1):\n",
        "  new_data = []\n",
        "  if max_length==-1: \n",
        "    # get max sequence length in data \n",
        "    for doc in mini_batch:\n",
        "      if max_length < len(doc):\n",
        "        max_length = len(doc)\n",
        "\n",
        "  for i, doc in enumerate(mini_batch):\n",
        "    if len(doc) <= max_length:\n",
        "      new_doc = doc + [padding_id]*(max_length - len(doc))\n",
        "    else:\n",
        "      new_doc = doc[:max_length]\n",
        "    new_data.append(list(new_doc))\n",
        "\n",
        "  return np.array(new_data) \n",
        " \n",
        "# padding and cut some words in sentence whose lengh greater MAX_DOC_LENGTH\n",
        "MAX_DOC_LENGTH = 300\n",
        "padding_train_data = padding_word(train_data, max_length=MAX_DOC_LENGTH)\n",
        "padding_test_data = padding_word(test_data, max_length=MAX_DOC_LENGTH)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(ReutersDataset(torch.LongTensor(padding_train_data[300:]),\n",
        "                                                         torch.LongTensor(train_labels[300:]), \n",
        "                                                         device=None), \n",
        "                                          batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "validloader = torch.utils.data.DataLoader(ReutersDataset(torch.LongTensor(padding_train_data[:300]),\n",
        "                                                         torch.LongTensor(train_labels[:300]), \n",
        "                                                         device=None), \n",
        "                                          batch_size=32,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(ReutersDataset(torch.LongTensor(padding_test_data),\n",
        "                                                         torch.LongTensor(test_labels), \n",
        "                                                         device=None), \n",
        "                                          batch_size=32,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "# define model + optimize method \n",
        "model = CNNText(vocab_size=VOCAB_SIZE, emb_dim=200, class_num=D_out)\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        " \n",
        "# training model \n",
        "model = train(model, criterion, optimizer, trainloader, validloader, N_EPOCHS = 8)\n",
        "\n",
        "# evaluate\n",
        "test_loss, test_acc, lb_predictions = evaluate(model, testloader)\n",
        "print('- test loss: %.3f \\n- test acc: %.3f' % (test_loss, test_acc*100))\n",
        "\n",
        "print(\"- 30 first predictions + gold labels:\")\n",
        "print(lb_predictions[:30])\n",
        "print(test_labels[:30])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNNText(\n",
            "  (embed): Embedding(10000, 200)\n",
            "  (convs1): ModuleList(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 200), stride=(1, 1))\n",
            "    (1): Conv2d(1, 32, kernel_size=(4, 200), stride=(1, 1))\n",
            "    (2): Conv2d(1, 32, kernel_size=(5, 200), stride=(1, 1))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.05, inplace=False)\n",
            "  (fc1): Linear(in_features=96, out_features=46, bias=True)\n",
            ")\n",
            "[Epoch 1/8, Iter   200] train loss: 2.007\n",
            "[Epoch 1, Iter   200] valid loss: 1.329 - acc: 0.703\n",
            "[Epoch 2/8, Iter   200] train loss: 1.079\n",
            "[Epoch 2, Iter   200] valid loss: 0.980 - acc: 0.800\n",
            "[Epoch 3/8, Iter   200] train loss: 0.729\n",
            "[Epoch 3, Iter   200] valid loss: 0.805 - acc: 0.833\n",
            "[Epoch 4/8, Iter   200] train loss: 0.489\n",
            "[Epoch 4, Iter   200] valid loss: 0.753 - acc: 0.827\n",
            "[Epoch 5/8, Iter   200] train loss: 0.340\n",
            "[Epoch 5, Iter   200] valid loss: 0.758 - acc: 0.830\n",
            "[Epoch 6/8, Iter   200] train loss: 0.236\n",
            "[Epoch 6, Iter   200] valid loss: 0.776 - acc: 0.830\n",
            "[Epoch 7/8, Iter   200] train loss: 0.185\n",
            "[Epoch 7, Iter   200] valid loss: 0.828 - acc: 0.843\n",
            "[Epoch 8/8, Iter   200] train loss: 0.171\n",
            "[Epoch 8, Iter   200] valid loss: 0.790 - acc: 0.843\n",
            "Finished Training\n",
            "- test loss: 0.954 \n",
            "- test acc: 81.389\n",
            "- 30 first predictions + gold labels:\n",
            "tensor([ 3, 10,  1,  4, 13,  3,  3,  3,  3,  3,  1,  4,  1,  3,  1, 11,  4,  3,\n",
            "        19,  3,  8,  3,  3,  3,  9,  3,  4,  6, 10,  3], device='cuda:0')\n",
            "[ 3 10  1  4  4  3  3  3  3  3  5  4  1  3  1 11 23  3 19  3  8  3  3  3\n",
            "  9  3  4  6 10  3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6XIRVmX3rX9",
        "colab_type": "code",
        "outputId": "7e70faaa-f4be-49a6-f8f5-5ac87accf6d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        }
      },
      "source": [
        "\n",
        "# define padding method \n",
        "def padding_word(mini_batch,  padding_id = 0, max_length=-1):\n",
        "  new_data = []\n",
        "  if max_length==-1: \n",
        "    # get max sequence length in data \n",
        "    for doc in mini_batch:\n",
        "      if max_length < len(doc):\n",
        "        max_length = len(doc)\n",
        "\n",
        "  for i, doc in enumerate(mini_batch):\n",
        "    if len(doc) <= max_length:\n",
        "      new_doc = doc + [padding_id]*(max_length - len(doc))\n",
        "    else:\n",
        "      new_doc = doc[:max_length]\n",
        "    new_data.append(list(new_doc))\n",
        "\n",
        "  return np.array(new_data) \n",
        " \n",
        "# padding and cut some words in sentence whose lengh greater MAX_DOC_LENGTH\n",
        "MAX_DOC_LENGTH = 150\n",
        "padding_train_data = padding_word(train_data, max_length=MAX_DOC_LENGTH)\n",
        "padding_test_data = padding_word(test_data, max_length=MAX_DOC_LENGTH)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(ReutersDataset(torch.LongTensor(padding_train_data[300:]),\n",
        "                                                         torch.LongTensor(train_labels[300:]), \n",
        "                                                         device=None), \n",
        "                                          batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "validloader = torch.utils.data.DataLoader(ReutersDataset(torch.LongTensor(padding_train_data[:300]),\n",
        "                                                         torch.LongTensor(train_labels[:300]), \n",
        "                                                         device=None), \n",
        "                                          batch_size=32,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(ReutersDataset(torch.LongTensor(padding_test_data),\n",
        "                                                         torch.LongTensor(test_labels), \n",
        "                                                         device=None), \n",
        "                                          batch_size=32,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "# define model + optimize method  vocab_size, embedding_dim, hidden_dim, output_size\n",
        "model = RNNText(vocab_size=VOCAB_SIZE, emb_dim=100, hidden_dim=100, class_num=D_out)\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        " \n",
        "# training model \n",
        "model = train(model, criterion, optimizer, trainloader, validloader, N_EPOCHS = 15)\n",
        "\n",
        "# evaluate\n",
        "test_loss, test_acc, lb_predictions = evaluate(model, testloader)\n",
        "print('- test loss: %.3f \\n- test acc: %.3f' % (test_loss, test_acc*100))\n",
        "\n",
        "print(\"- 30 first predictions + gold labels:\")\n",
        "print(lb_predictions[:30])\n",
        "print(test_labels[:30])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNNText(\n",
            "  (embedding): Embedding(10000, 100)\n",
            "  (lstm): LSTM(100, 100, batch_first=True, bidirectional=True)\n",
            "  (hidden2out): Linear(in_features=100, out_features=46, bias=True)\n",
            "  (softmax): LogSoftmax()\n",
            "  (dropout_layer): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1/15, Iter   200] train loss: 2.038\n",
            "[Epoch 1, Iter   200] valid loss: 1.659 - acc: 0.577\n",
            "[Epoch 2/15, Iter   200] train loss: 1.585\n",
            "[Epoch 2, Iter   200] valid loss: 1.507 - acc: 0.610\n",
            "[Epoch 3/15, Iter   200] train loss: 1.368\n",
            "[Epoch 3, Iter   200] valid loss: 1.326 - acc: 0.680\n",
            "[Epoch 4/15, Iter   200] train loss: 1.104\n",
            "[Epoch 4, Iter   200] valid loss: 1.135 - acc: 0.747\n",
            "[Epoch 5/15, Iter   200] train loss: 0.909\n",
            "[Epoch 5, Iter   200] valid loss: 1.057 - acc: 0.780\n",
            "[Epoch 6/15, Iter   200] train loss: 0.765\n",
            "[Epoch 6, Iter   200] valid loss: 1.077 - acc: 0.767\n",
            "[Epoch 7/15, Iter   200] train loss: 0.656\n",
            "[Epoch 7, Iter   200] valid loss: 0.971 - acc: 0.767\n",
            "[Epoch 8/15, Iter   200] train loss: 0.562\n",
            "[Epoch 8, Iter   200] valid loss: 1.009 - acc: 0.790\n",
            "[Epoch 9/15, Iter   200] train loss: 0.474\n",
            "[Epoch 9, Iter   200] valid loss: 1.010 - acc: 0.797\n",
            "[Epoch 10/15, Iter   200] train loss: 0.403\n",
            "[Epoch 10, Iter   200] valid loss: 0.987 - acc: 0.790\n",
            "[Epoch 11/15, Iter   200] train loss: 0.355\n",
            "[Epoch 11, Iter   200] valid loss: 1.050 - acc: 0.790\n",
            "[Epoch 12/15, Iter   200] train loss: 0.298\n",
            "[Epoch 12, Iter   200] valid loss: 1.061 - acc: 0.787\n",
            "[Epoch 13/15, Iter   200] train loss: 0.276\n",
            "[Epoch 13, Iter   200] valid loss: 1.040 - acc: 0.783\n",
            "[Epoch 14/15, Iter   200] train loss: 0.248\n",
            "[Epoch 14, Iter   200] valid loss: 1.047 - acc: 0.790\n",
            "[Epoch 15/15, Iter   200] train loss: 0.219\n",
            "[Epoch 15, Iter   200] valid loss: 1.067 - acc: 0.797\n",
            "Finished Training\n",
            "- test loss: 1.244 \n",
            "- test acc: 76.581\n",
            "- 30 first predictions + gold labels:\n",
            "tensor([ 4, 10, 41,  4, 13,  3,  3,  3,  3,  3,  1,  4,  1,  3,  1, 11, 25,  3,\n",
            "        19,  3,  8,  3,  3,  3,  9,  3,  4,  6, 10,  3], device='cuda:0')\n",
            "[ 3 10  1  4  4  3  3  3  3  3  5  4  1  3  1 11 23  3 19  3  8  3  3  3\n",
            "  9  3  4  6 10  3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76fzIT1Iao2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}